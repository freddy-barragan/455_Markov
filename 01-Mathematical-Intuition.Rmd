# Mathematical Intuition

Markov Chains are cool! And Hidden Markov Models are even cooler! So, let's make sure you can get at the cool-stuff by starting off with the necessary basics. In this section, we'll review conditional probabilities & set up the basis for Hidden Markov Models by getting comfortable with chains first.

\

---

## Conditional Probability & Bayes Rule

### Basic Concepts {-}

Probability, as a field, formalizes how we predict events with some equally beautiful & ugly notation, inutitive concepts, and complex mathematical principles. But the premise is simple: by ascribing a numeric value to the outcomes of an event happening, we can abstract the real-world and study it with math. 

The process of ascribing numeric values to the outcome of an event is called a mapping & by mapping all possible probabilities of an event's outcomes we create a **random variable**.

> **NOTE:** This can be confusing! The "random" part of the word doesn't mean all outcomes have an equal chance of happening, really it means that within an event there are multiple possible outcomes.

\

**Example: Weather & Temperature**

Let's say that the weather is the event, $W$, whose only **outcomes** are sunny ($w_s$), rainy ($w_r$) or cloudy($w_c$). 
By mapping numeric values (e.g. probabilities) to the outcomes we can turn $W$ into a random variable. Below, we list all mappings in the **probability mass function**, $p_{(W)}(w)$

$$\begin{aligned} 
p_{(W)}(w)  &= \begin{cases}
    0.7, & \text{for } w_s \\
    0.1, & \text{for } w_r \\
    0.2, & \text{for } w_c \\
    0.0, & \text{otherwise}
\end{cases}
\end{aligned}$$

\

Now, let's that the daily temperature (F°) is the event $T$. Normally, we could say that $T$ follows a normal distribution with $\mu = 75 \textbf{ F°}$ and $\sigma^2 = 625 \textbf{ F°}$, since its a continuous variable. So, $T \sim N(75, 625)$ and would look something like this

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE, fig.width=5/1.5, fig.height=3/1.5}
library(tidyverse)
ggplot(data = data.frame(x = c(-25, 175)), aes(x)) +
  stat_function(fun = dnorm, 
                n = 1000,
                args = list(mean = 75, sd = 25), 
                color="black", size=1.5) + 
  labs(y="Probabilities", title="Temperature (F°)", x="")+
  scale_x_continuous(breaks = seq(0,150, by=25))+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

\

Instead, let's say that $T$ is a discrete random variable whose only potential outcomes are cold ($t_c$), fresh ($t_n$), or hot ($t_h$). Then, $T$ has the probability mass function

$$\begin{aligned} 
p_{(T)}(t)  &= \begin{cases}
    0.155, & \text{for } t_c \\
    0.5, & \text{for } t_f \\
    0.345, & \text{for } t_h \\
    0.0, & \text{otherwise}
\end{cases}
\end{aligned}$$

\

Below are two graphs summarizing what we know so far about $W$ and $T$ 

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE}
data <- data.frame(probability = c(0.7, 0.1,0.2), temperature = c("Sunny", "Rainy", "Cloudy"))

p1<-ggplot(data, aes(x=temperature, y=probability, fill=temperature)) +
  geom_col()+
  scale_fill_manual(values=c("#D5D5D3", "#85D4E3",  "#FAD77B"))+
  labs(y="Probabilities", title="Weather", x="")+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE}
data <- data.frame(probability = c(0.155, .5,.345), temperature = c("Cold", "Fresh", "Hot"))

p2 <- ggplot(data, aes(x=temperature, y=probability, fill=temperature)) +
  geom_col()+
  scale_fill_manual(values=c("#3B9AB2", "#00A08A",  "#F1BB7B"))+
  labs(y="Probabilities", title="Temperature", x="")+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE,  fig.width=5, fig.height=3/1.5}
library(gridExtra)
grid.arrange(p1,p2, ncol=2)
```

But what happens if the weather depends on the temperature? 

\
\

### Conditional Probabilities {-}

Let's study the two arbitrary events $A$, $B$ & learn some definitions about probability.

**Conditional Probability:** 
The conditional probabilities of $A$'s, given $B$ and $B$, given $A$ are written below.
$$P(A|B) \hspace{1 in}P(B|A)$$
\

**Independence:** 
We say that the two events, $A$ & $B$ are independent if the conditional probabilities provide us no new-information about either event. So, 

$$P(A|B) = P(A) \hspace{1 in}P(B|A) = P(B)$$
\

**Joint Probability:** 
The probability of two events, $A$ & $B$ happening at the same time is called a **joint probability** and is typically denoted by $P(A\cap B)$. It is calculated below.

$$\begin{aligned} 
P(A\cap B) &= P(A|B) \cdot P(B)\\
&= P(A) \cdot P(B) && \text{if independent}
\end{aligned}$$

\
\


It's generally true that the weather on a particular day, depends on the temperature. This implies that $W$ and $T$ are conditional events with conditional probabilities. So, 

$$P(W\mid T) \neq P(W)$$

### Bayes' Rule & LOTP {-}

Conceived by Reverend Thomas Bayes in the 18th Century, posthumously published by his friend Richard Price, and then formalized into an equation by Pierre-Simon Laplace, Bayes' Rule is a cornerstone equation in modern statistics & probability. I've written it below

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}=\frac{P(A)\cdot P(B|A)}{P(B)}$$

> **NOTE**: Above you'll notice that we're looking at the probability of $A$ & $B$, divided by the probability of $B$. We are dividing by $P(B)$ to normalize & isolate the probability of $A$, under the conditions we observe $B$.

> **Quick Check:** If $A$ & $B$ are independent, how would you further simplify the numerator of Bayes' Rule?


The denominator of Bayes' Rule, $P(B)$ is called the **marginal probability** of $B$.

The marginal probability can either be

- given 
- computed with the **law of total probability** or (**LOTP**). 

**LOTP** states that
$$\begin{aligned}
P(B) = \sum_{i=1}^n P(B \cap A_i) &= P(B \cap A_1) + P(B\cap A_2) + \dots + P(B\cap A_n) \\
&= P(B \mid A_1)P(A_1) + P(B\mid A_2)P(A_2) + \dots + P(B\mid A_n)P(A_n) \\
\end{aligned}$$

> **Note:** Looks scary! Really it's like calculating the probability of $B$ in each subcategory of $A$, multiplying by the probability of that sub-$A$, then adding it all together.

\

---

## Markov Chains

**Stochastic processes** are events which have some element of randomness in their outcomes. The amount of 'randomness' & the type of events can vary depending on the context. In turn, studying the properties of stochastic processes often requires many different techniques which go well beyond the boundaries of statistics. And with a litany of applications across so many domains of knowledge, studying stochastic processes also involves many techniques from Physics, Linguistics, Sociology, Public Health, Geography & more. 

> **Note:** There is *some* nuance in the language we use to describe randomness, probabilistic, and stochastic, but it is murky. So, for now, let's stick with the above definition & some enjoy the interchangeability between random, stochastic, and probabilistic.

So, this section will only be a tiny snippet of the wide topics covered in studying stochastic processes.

### Intuition {-}

Markov chains are a subclass of stochastic processes which describe partially-random events occurring over time, typically in succession. We will formalize this definition soon, but for now let's talk about the weather.

*Example: Weather*

Let's ignore the fact that temperature determines weather. Instead, let's assume that we can compute the probability of tomorrow's weather by only looking at today's. This results in three important ideas:

- General weather patterns before today are irrelevant when predicting the tomorrow's weather. This is an example of a Markov process, more specifically a Markov Chain.
- The outcomes of the weather are rainy, sunny, or cloudy. These are examples of states.
- The probability of tomorrow's weather depends on whether it was rainy, sunny, or cloudy to day. Meaning there are probabilities associated with tomorrow's events, strictly defined by today's. These are examples of transition probabilities.

Let's be rigorous now!

### Markov Chain {-}

Let X_i successive time-indexed events is a **Markov Chain**. There are 
   
**Visualizing a Markov Chain**
   - Will Hipson's [Dot Visualization](https://willhipson.netlify.app/post/markov-sim/markov_chain/) 
   
```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE}
library(wesanderson)
library(plotly)
set.seed(200322)
markov2 <- function(n = 10, iter = 15, start_probs = c(.5, .2, .3), 
                    trans_probs = matrix(c(.3, .1, .7, 
                                           .2, .5, .2,
                                           .5, .4, .1), ncol = 3),
                    plot_prob = TRUE) { # Here we have a full non-zero transition matrix.
  #Check to see if probability entries are valid.
  if(sum(start_probs) != 1 | any(start_probs < 0)) 
    stop("start_probs must be non-negative and sum to 1.")
  if(any(apply(trans_probs, 1, sum) != 1))
    stop("trans_probs matrix rows must sum to 1.")
  if(any(trans_probs < 0))
    stop("elements of trans_probs must be non-negative")

  dt <- matrix(NA, nrow = iter, ncol = n) # Initialize matrix to hold iterations
  # Run chain
  for(i in 1:iter) {
    for(j in 1:n) {
      if(i == 1) { # if we're at the beginning of the simulation
        dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = start_probs)
      } else {
        if(dt[i - 1, j] == 0) { # if the previous state was 0
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,1])
        } else if (dt[i - 1, j] == 1) { # if the previous state was 1
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,2])
        } else {
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,3])
        }
      }
    }
  }
  
  # Return chain as dataframe
  return(as.data.frame(dt))
}

df2 <- markov2()


df_long2 <- df2 %>%
  rowid_to_column(var = "iter") %>%
  pivot_longer(cols = V1:V10) %>%
  group_by(name) %>%
  mutate(x = value + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         y = value %% 2 + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         initial = value[1],
         is_0 = ifelse(value == 0, TRUE, FALSE),
         is_1 = ifelse(value == 1, TRUE, FALSE)) %>%
  group_by(iter) %>%
  mutate(prop0 = mean(is_0),
         prop1 = mean(is_1),
         prop2 = 1 - prop0 - prop1) %>%
  ungroup()

anim2 <- df_long2 %>%
  plot_ly(
    x = ~x,
    y = ~y,
    color = ~factor(initial),
    size = 5,
    colors =as.vector(wes_palette("GrandBudapest2", n=3, type="discrete")),
    frame = ~iter,
    type = 'scatter',
    mode = 'markers',
    showlegend = FALSE
  )

anim2 <- anim2 %>%
  add_text(x = 0, y = .35, text = ~prop0, textfont = list(color = "#E6A0C4", size = 24, opacity = .6)) %>%
  add_text(x = 1, y = 1.25, text = ~prop1, textfont = list(color = "#C6CDF7", size = 24, opacity = .6)) %>%
  add_text(x = 2, y = .35, text = ~prop2, textfont = list(color = "#D8A499", size = 24, opacity = .6))

ax2 <- list(
  zeroline = FALSE,
  showline = FALSE,
  showticklabels = FALSE,
  showgrid = FALSE,
  title = ""
)

anim2 <- anim2 %>%
  layout(xaxis = ax2, yaxis = ax2) %>%
  animation_opts(redraw = FALSE) %>%
  animation_slider(hide = TRUE) %>%
  animation_button(x = .6, y = .10, showactive = FALSE, label = "Run Simulation") %>%
  config(displayModeBar = FALSE, scrollZoom = FALSE, showTips = FALSE)
  
anim2


```

   
   - embedding [Victor Powell's Website](https://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.5%2C0.5%5D%2C%5B0.5%2C0.5%5D%5D%7D).
   
 
## Video Resources

<iframe width="373" height="210" src="https://www.youtube.com/embed/VCyJGp6Enxg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

CIATION

<iframe width="373" height="210" src="https://www.youtube.com/embed/JHwyHIz6a8A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

CITATION
   

<br>
<br>

---


**References**

Hipson, Will. 2020. “Visualizing A Markov Chain.” Will Hipson’s Personal Website. March 23, 2020. https://willhipson.netlify.app/post/markov-sim/markov_chain/.

Johnson, Alicia, Miles Ott, and Mine Dogucu. 2020. “Bayes’ Rule.” In  Bayes Rules! An Introduction to Bayesian Modeling with R . https://www.bayesrulesbook.com.

Kang, Eugine. 2017. “Hidden Markov Model.” Medium. August 31, 2017. https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9.

Lay, David C. 2012. “Applications to Markov Chains.” In Linear Algebra and Its Applications, 4th ed., 253–62. Boston: Pearson College Division.

Powell, Victor, and Lewis Lehe. 2014. “Markov Chains Explained Visually.” Explained Visually. November 7, 2014. https://setosa.io/ev/markov-chains/.
 