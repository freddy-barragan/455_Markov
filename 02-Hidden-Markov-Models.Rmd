# Hidden Markov Models

In this section, we'll go through the theoretical definitions & algorithms associated with Hidden Markov Models (**HMM**s) and finally get at proving the claim that they're cooler than Markov Chains. Let's begin with some applied motivation.

## Motivation

Previously, we wanted to study the weather. Let's up the stakes and now look at how the weather can determine whether or not a small frog-family survives the week in California. 

<center>
![](images/froggy.jpg)
</center>

If you were a frog, you wouldn't be able to check the weather before leaving your tree hide-- you wouldn't even be able to read!-- but you do know that if you left while it was 

- sunny, you'd dry up in an instant & die with 100% certainty
- cloudy, you would have a 75% chance of living (and eating a nice bug) with a 25% chance of dying due to predators in the area
- rainy, you would have a 95% chance of living (and eating a nice bug) with a 5% chance of dying due to predators in the area

Let's say your frog-dad went out and came back (with a nice bug), but now you're very curious about the weather outside and want to predict it somehow.

Since we, as non-frog statisticians, know that the weather is a Markov Chain, we can say this frog-family has stumbled into a Hidden Markov Model. In the following section, we will establish what that implies theoretically & for our little frog family.

## Definitions


Hidden Markov Models are a class of probabilistic models which model Markov processes whose outcomes cannot be solely predicted using the observed, discrete, events ($Y_n$), as is the case for Markov chains. Instead, we must use the set of hidden, discrete, events ($X_n$). This implies 4 things for the set of events, $X_n$ & $Y_n$:

- Discreteness: Both $X_n$ and $Y_n$ are sequential, discrete, random variables.
- Markov Process: $X_n$ must have a Markov process. That is, $P(X_{n+1}|X_1, ..., X_n) = P(X_{n+1}|X_n)$
- Hiddenness: $X_n$ are not observable.
- Outcome Probabilities: The outcome probabilities of a state at a certain point are solely dependent on hidden states at the same point. That is $P(Y_{n}|Y_i, ..., Y_n, X_1, ..., X_n) = P(Y_{n}|X_n)$


 + Definition of **Hidden Markov Model**
 Definition of Hidden States
   - Definition of Observation Likelihoods
   - Visual Intuition with family data and `seqHMM`
   
## The Forward-Backward Algorithm
 + The Forward-Backward Algorithm
   - Motivation: Given hidden states, find the likelihood of the observations
   - Define connection between Bayes rule
   - Define Joint Probability
   - Walkthrough of Algorithm

## Limitations 

   



---


**References**

Degirmenci, Alperen. 2014. “Introduction to Hidden Markov Models.” Harvard University. https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf.

Eddy, Sean. 2004. “What Is a Hidden Markov Model?,” October, 1315–1316. https://www.nature.com/articles/nbt1004-1315#citeas.

Helske, Satu, and Jouni Helske. 2019. “Mixture Hidden Markov Models for Sequence Data: The SeqHMM Package in R” 88: 1–32. http://dx.doi.org/10.18637/jss.v088.i03.

Jurafsky, Dan, and James Martin. 2020. “Hidden Markov Models.” In Speech and Language Processing, 3rd ed. https://web.stanford.edu/~jurafsky/slp3/A.pdf.

Kang, Eugine. 2017. “Hidden Markov Model.” Medium. August 31, 2017. https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9.


\
